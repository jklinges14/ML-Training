{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "Adapted from Chapter 8 of An Introduction to Statistical Learning\n",
    "\n",
    "Motivation: Why are we learning about decision trees?\n",
    "\n",
    "Useful for both regression and classification problems\n",
    "Widely used\n",
    "Basis for more sophisticated models\n",
    "Have a different way of \"thinking\" than the other models we have studied\n",
    "Part 1: Regression trees\n",
    "Baseball player salary data:\n",
    "\n",
    "Years (x-axis): number of years playing in the major leagues\n",
    "Hits (y-axis): number of hits in the previous year\n",
    "Salary (color): low salary is blue/green, high salary is red/yellow\n",
    "Salary data\n",
    "\n",
    "Group exercise:\n",
    "\n",
    "The data above is our training data.\n",
    "We want to build a model that predicts the Salary of future players based on Years and Hits.\n",
    "We are going to \"segment\" the feature space into regions, and then use the mean Salary in each region as the predicted Salary for future players.\n",
    "Intuitively, you want to maximize the similarity (or \"homogeneity\") within a given region, and minimize the similarity between different regions.\n",
    "Rules for segmenting:\n",
    "\n",
    "You can only use straight lines, drawn one at a time.\n",
    "Your line must either be vertical or horizontal.\n",
    "Your line stops when it hits an existing line.\n",
    "Salary regions\n",
    "\n",
    "Above are the regions created by a computer:\n",
    "\n",
    "$R_1$: players with less than 5 years of experience, mean Salary of \\$166,000\n",
    "$R_2$: players with 5 or more years of experience and less than 118 hits, mean Salary of \\$403,000\n",
    "$R_3$: players with 5 or more years of experience and 118 hits or more, mean Salary of \\$846,000\n",
    "Note: Years and Hits are both integers, but the convention is to use the midpoint between adjacent values to label a split.\n",
    "\n",
    "These regions are used to make predictions on out-of-sample data. Thus, there are only three possible predictions! (Is this different from how linear regression makes predictions?)\n",
    "\n",
    "Below is the equivalent regression tree:\n",
    "\n",
    "Salary tree\n",
    "\n",
    "The first split is Years < 4.5, thus that split goes at the top of the tree. When a splitting rule is True, you follow the left branch. When a splitting rule is False, you follow the right branch.\n",
    "\n",
    "For players in the left branch, the mean Salary is \\$166,000, thus you label it with that value. (Salary has been divided by 1000 and log-transformed to 5.11.)\n",
    "\n",
    "For players in the right branch, there is a further split on Hits < 117.5, dividing players into two more Salary regions: \\$403,000 (transformed to 6.00), and \\$846,000 (transformed to 6.74).\n",
    "\n",
    "Salary tree annotated\n",
    "\n",
    "What does this tree tell you about your data?\n",
    "\n",
    "Years is the most important factor determining Salary, with a lower number of Years corresponding to a lower Salary.\n",
    "For a player with a lower number of Years, Hits is not an important factor determining Salary.\n",
    "For a player with a higher number of Years, Hits is an important factor determining Salary, with a greater number of Hits corresponding to a higher Salary.\n",
    "Question: What do you like and dislike about decision trees so far?\n",
    "\n",
    "Building a regression tree by hand\n",
    "Your training data is a tiny dataset of used vehicle sale prices. Your goal is to predict price for testing data.\n",
    "\n",
    "Read the data into a Pandas DataFrame.\n",
    "Explore the data by sorting, plotting, or split-apply-combine (aka group_by).\n",
    "Decide which feature is the most important predictor, and use that to create your first splitting rule.\n",
    "Only binary splits are allowed.\n",
    "After making your first split, split your DataFrame into two parts, and then explore each part to figure out what other splits to make.\n",
    "Stop making splits once you are convinced that it strikes a good balance between underfitting and overfitting.\n",
    "Your goal is to build a model that generalizes well.\n",
    "You are allowed to split on the same variable multiple times!\n",
    "Draw your tree, labeling the leaves with the mean price for the observations in that region.\n",
    "Make sure nothing is backwards: You follow the left branch if the rule is true, and the right branch if the rule is false.\n",
    "How does a computer build a regression tree?\n",
    "Ideal approach: Consider every possible partition of the feature space (computationally infeasible)\n",
    "\n",
    "\"Good enough\" approach: recursive binary splitting\n",
    "\n",
    "Begin at the top of the tree.\n",
    "For every feature, examine every possible cutpoint, and choose the feature and cutpoint such that the resulting tree has the lowest possible mean squared error (MSE). Make that split.\n",
    "Examine the two resulting regions, and again make a single split (in one of the regions) to minimize the MSE.\n",
    "Keep repeating step 3 until a stopping criterion is met:\n",
    "maximum tree depth (maximum number of splits required to arrive at a leaf)\n",
    "minimum number of observations in a leaf\n",
    "Demo: Choosing the ideal cutpoint for a given feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicle data\n",
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT7/master/data/vehicles_train.csv'\n",
    "train = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before splitting anything, just predict the mean of the entire dataset\n",
    "train['prediction'] = train.price.mean()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSE for those predictions\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "np.sqrt(metrics.mean_squared_error(train.price, train.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that calculates the RMSE for a given split of miles\n",
    "def mileage_split(miles):\n",
    "    lower_mileage_price = train[train.miles < miles].price.mean()\n",
    "    higher_mileage_price = train[train.miles >= miles].price.mean()\n",
    "    train['prediction'] = np.where(train.miles < miles, lower_mileage_price, higher_mileage_price)\n",
    "    return np.sqrt(metrics.mean_squared_error(train.price, train.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSE for tree which splits on miles < 50000\n",
    "print 'RMSE:', mileage_split(50000)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSE for tree which splits on miles < 100000\n",
    "print 'RMSE:', mileage_split(100000)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all possible mileage splits\n",
    "mileage_range = range(train.miles.min(), train.miles.max(), 1000)\n",
    "RMSE = [mileage_split(miles) for miles in mileage_range]\n",
    "\n",
    "# plot mileage cutpoint (x-axis) versus RMSE (y-axis)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(mileage_range, RMSE)\n",
    "plt.xlabel('Mileage cutpoint')\n",
    "plt.ylabel('RMSE (lower is better)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap: Before every split, this process is repeated for every feature, and the feature and cutpoint that produces the lowest MSE is chosen.\n",
    "\n",
    "Building a regression tree in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode car as 0 and truck as 1\n",
    "train['vtype'] = train.vtype.map({'car':0, 'truck':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "feature_cols = ['year', 'miles', 'doors', 'vtype']\n",
    "X = train[feature_cols]\n",
    "y = train.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a DecisionTreeRegressor (with random_state=1)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "treereg = DecisionTreeRegressor(random_state=1)\n",
    "treereg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use leave-one-out cross-validation (LOOCV) to estimate the RMSE for this model\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(treereg, X, y, cv=14, scoring='mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens when we grow a tree too deep?\n",
    "Left: Regression tree for Salary grown deeper\n",
    "Right: Comparison of the training, testing, and cross-validation errors for trees with different numbers of leaves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Salary tree grown deep\n",
    "The training error continues to go down as the tree size increases (due to overfitting), but the lowest cross-validation error occurs for a tree with 3 leaves.\n",
    "\n",
    "Tuning a regression tree\n",
    "Let's try to reduce the RMSE by tuning the max_depth parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different values one-by-one\n",
    "treereg = DecisionTreeRegressor(max_depth=1, random_state=1)\n",
    "scores = cross_val_score(treereg, X, y, cv=14, scoring='mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Or, we could write a loop to try a range of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of values to try\n",
    "max_depth_range = range(1, 8)\n",
    "\n",
    "# list to store the average RMSE for each value of max_depth\n",
    "RMSE_scores = []\n",
    "\n",
    "# use LOOCV with each value of max_depth\n",
    "for depth in max_depth_range:\n",
    "    treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
    "    MSE_scores = cross_val_score(treereg, X, y, cv=14, scoring='mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
    "\n",
    "# plot max_depth (x-axis) versus RMSE (y-axis)\n",
    "plt.plot(max_depth_range, RMSE_scores)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('RMSE (lower is better)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth=3 was best, so fit a tree using that parameter\n",
    "treereg = DecisionTreeRegressor(max_depth=3, random_state=1)\n",
    "treereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Gini importance\" of each feature: the (normalized) total reduction of error brought by that feature\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a tree diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a GraphViz file\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(treereg, out_file='tree_vehicles.dot', feature_names=feature_cols)\n",
    "\n",
    "# At the command line, run this to convert to PNG:\n",
    "# dot -Tpng tree_vehicles.dot -o tree_vehicles.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the internal nodes:\n",
    "\n",
    "samples: number of observations in that node before splitting\n",
    "mse: MSE calculated by comparing the actual response values in that node against the mean response value in that node\n",
    "rule: rule used to split that node (go left if true, go right if false)\n",
    "Reading the leaves:\n",
    "\n",
    "samples: number of observations in that node\n",
    "value: mean response value in that node\n",
    "mse: MSE calculated by comparing the actual response values in that node against \"value\"\n",
    "Making predictions for the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the testing data\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT7/master/data/vehicles_test.csv'\n",
    "test = pd.read_csv(url)\n",
    "test['vtype'] = test.vtype.map({'car':0, 'truck':1})\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Using the tree diagram above, what predictions will the model make for each observation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use fitted model to make predictions on testing data\n",
    "X_test = test[feature_cols]\n",
    "y_test = test.price\n",
    "y_pred = treereg.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSE\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSE for your own tree!\n",
    "y_test = [3000, 6000, 12000]\n",
    "y_pred = [0, 0, 0]\n",
    "from sklearn import metrics\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT7/master/data/titanic.csv'\n",
    "titanic = pd.read_csv(url)\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What special handling do we need to apply (if any) to the following columns?\n",
    "\n",
    "Survived: 1=survived, 0=passed away (response variable)\n",
    "Pclass: 1=first class, 2=second class, 3=third class\n",
    "What will happen if the tree splits on this feature?\n",
    "Sex: male or female\n",
    "Age: numeric value\n",
    "Embarked: C or Q or S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode female as 0 and male as 1\n",
    "titanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\n",
    "\n",
    "# fill in the missing values for age with the mean age\n",
    "titanic.Age.fillna(titanic.Age.mean(), inplace=True)\n",
    "\n",
    "# create three dummy variables, drop the first dummy variable, and store the two remaining columns as a DataFrame\n",
    "embarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked').iloc[:, 1:]\n",
    "\n",
    "# concatenate the two dummy variable columns onto the original DataFrame\n",
    "titanic = pd.concat([titanic, embarked_dummies], axis=1)\n",
    "\n",
    "# print the updated DataFrame\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "feature_cols = ['Pclass', 'Sex', 'Age', 'Embarked_Q', 'Embarked_S']\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a classification tree with max_depth=3 on all data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "treeclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a GraphViz file\n",
    "export_graphviz(treeclf, out_file='tree_titanic.dot', feature_names=feature_cols)\n",
    "\n",
    "# At the command line, run this to convert to PNG:\n",
    "# dot -Tpng tree_titanic.dot -o tree_titanic.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the feature importances\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Comparing decision trees with other models¶\n",
    "Advantages of decision trees:\n",
    "\n",
    "Can be used for regression or classification\n",
    "Can be displayed graphically\n",
    "Highly interpretable\n",
    "Can be specified as a series of rules, and more closely approximate human decision-making than other models\n",
    "Prediction is fast\n",
    "Features don't need scaling\n",
    "Automatically learns feature interactions\n",
    "Tends to ignore irrelevant features\n",
    "Non-parametric (will outperform linear models if relationship between features and response is highly non-linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disadvantages of decision trees:\n",
    "\n",
    "Performance is (generally) not competitive with the best supervised learning methods\n",
    "Can easily overfit the training data (tuning is required)\n",
    "Small variations in the data can result in a completely different tree (high variance)\n",
    "Recursive binary splitting makes \"locally optimal\" decisions that may not result in a globally optimal tree\n",
    "Doesn't tend to work well if the classes are highly unbalanced\n",
    "Doesn't tend to work well with very small datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
